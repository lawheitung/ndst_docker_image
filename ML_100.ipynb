{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#8735fb'> **Airline Delays - ML Workflow** </font> \n",
    "> <font color='#8735fb'> [ single CPU/ GPU ] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAPIDS aims to contribute to data science by building GPU accelerated versions of open-source data science libraries. \n",
    "\n",
    "With RAPIDS and GPUs the inherent parallelism of the data science workflow is exposed to massive compute making possible a qualitative improvement in the life of a practicing data scientist, as well as substantial speedups for large scale machine learning in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/airline_dataset.png' width='1250px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> \n",
    "<a href='https://d7vw40z4bofef.cloudfront.net/static/2.37.07-web19/images/service/isometric/flight.svg'>image source</a> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, pandas\n",
    "import sklearn, xgboost        # python data science stack\n",
    "import cupy, cudf, cuml        # RAPIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teach_ML, benchmark    # helper functions, benchmaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib               # dynamic code reloading\n",
    "importlib.reload( teach_ML ); \n",
    "importlib.reload( benchmark );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#8735fb'> Motivation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll train a machine learning model to anticipate flight delays -- i.e., to determine if a flight will arrive more than 15 minutes past its scheduled time. Accurately predicing late flights is an application of machine learning that could be used to help improve airport operation, carrier logistics, and/or consumer travel planning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we go forward with the workflow we'll narrate what's happening with the data as it's prepared for ingestion by our model. Along the way we'll show the steps we take with code written using the data science stack of `pandas`, `numpy`, and `sklearn` as well as code using their RAPIDS euqivalents `cupy`, `cudf`, and `cuml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **1. Performance Tracking**\n",
    "\n",
    "> **2. Data Ingestion**\n",
    "\n",
    "> **3. ETL**\n",
    "-> handle missing -> encode non-numerics -> split\n",
    "\n",
    "> **3. Explore**\n",
    "-> cross-correlation \n",
    "-> visualization [ TODO: dynamic plot ] \n",
    "-> finalize feature selection \n",
    "\n",
    "> **4. Train Classifier**\n",
    "-> XGBoost vs RF\n",
    "\n",
    "> **5. Inference**\n",
    "-> FIL\n",
    "\n",
    "> **6. Advanced Topics / Extensions [ optional ]**\n",
    "-> interactive multi-plot viz, onramp to DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#8735fb'> **Dataset** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of our analysis will be domestic carrier on-time reporting data that has been kept for decades by the U.S. Bureau of Transportation.\n",
    "\n",
    "This rich source of data allows us to scale, so while in this notebook (ML_100.ipynb) we only use 1 GPU and 1 year of data, in the next notebook (ML200.ipynb) we'll use 10 years of data and multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dataset**: [US.DoT - Reporting Carrier On-Time Performance, 1987-Present](https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The public dataset contains logs/features about flights in the United States (17 airlines) including:\n",
    "\n",
    "* locations and distance  ( `Origin`, `Dest`, `Distance` )\n",
    "* airline / carrier ( `Reporting_Airline` )\n",
    "* scheduled departure and arrival times ( `CRSDepTime` and `CRSArrTime` )\n",
    "* actual departure and arrival times ( `DpTime` and `ArrTime` )\n",
    "* difference between scheduled & actual times ( `ArrDelay` and `DepDelay` )\n",
    "* binary encoded version of late, aka our target variable ( `ArrDelay15` )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: We limit this first workflow to a single calendar year (Jan - Dec of 2019) which is about 7.5M flights. As an extension we also look at how you might train a model ensemble to handle more recent data (see [impacts of COVID-19](https://www.bts.gov/data-spotlight/march-day-day-how-flight-cancellations-rose-17)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#8735fb'> 1. Performance Tracking </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we run a piece of our pipeline we'll be able to benchmark its performance on the compute type of our choice and update the value in a log dictionary. Let's start by initializing an empty dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='#8735fb'> Compute Choice </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the option of selecting to run either only-GPU or both CPU and GPU code cells. Use the widget below to make your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_choice = ipywidgets.ToggleButtons( description = 'compute:', options = ['GPU', 'GPU & CPU'] )\n",
    "\n",
    "display( compute_choice )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='#8735fb'> Benchmarking Context </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll wrap our code in a context manager object that allows us to \n",
    "* gracefully handle error catching \n",
    "* updates our result `log` with the duration of the code body\n",
    "* allows us to specify the compute requirements of the execution block [ and match the user's `compute_choice` ]\n",
    "\n",
    "For more on how the python with statement and context managers work see [this great writeup](https://effbot.org/zone/python-with-statement.htm), or [the python docs](https://docs.python.org/3/reference/compound_stmts.html#the-with-statement), and/or check out the code in `benchmark.py`\n",
    "\n",
    "An example of how we use the benchmarking context manager is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "In[0]: with benchmark.GPU ( log, 'ingestion', compute_choice ) as GPU_context:\n",
    "  ...:     if GPU_context.execute_block == True:\n",
    "  ...:         # execution body\n",
    "  ...:         data = cudf.read_csv( data_dir + airline_stats, index_col = 0)\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#8735fb'> 2. Data Ingestion </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#8735fb'> Loading Airline Dataset [2019] <font>\n",
    "> customize for your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/workspace/ml/data/'\n",
    "airline_stats = '2019_airlines.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = str(data_dir + airline_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cudf.DataFrame()\n",
    "data_cpu = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#8735fb'> 2.1. CPU Ingestion </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark.CPU ( log, 'ingestion', compute_choice ) as CPU_context:\n",
    "    if CPU_context.execute_block == True:\n",
    "        \n",
    "        data_cpu = pandas.read_csv( csv_filename, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#8735fb'> 2.2. GPU Ingestion </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark.GPU ( log, 'ingestion', compute_choice ) as GPU_context:\n",
    "    if GPU_context.execute_block == True:\n",
    "        \n",
    "        data = cudf.read_csv( csv_filename, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teach_ML.compare_speedups(log);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#8735fb'> 2.3. Validation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at what has been loaded into our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( f'data shape on GPU : {data.shape} \\n'\n",
    "       f'data shape on CPU : {data_cpu.shape}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the shape of our data tells us, this is a classic tabular dataset -- i.e., very tall and narrow (almost 300,000 times longer than it is wide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head().to_pandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cpu=data_cpu.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#8735fb'>[ Optional ] Augmentation</font>\n",
    "\n",
    "Modern data scientists have an ever growing arsenal of public datasets that can be combined with their base data. In our workflow, we'll add some additional information by adding in airport locations and the human readable airline carrier names (rather than their coded versions). These additions will help us in upstream interpretation and visualization. \n",
    "\n",
    "More spefically we'll merge in\n",
    "> full-string names of airlines matched on the carrier codes in the baseline dataset [`carriers.csv`]\n",
    "\n",
    "> airport locations (lat,lng) matches on airport codes in the baseline dataset [`airports.csv`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_choice = ipywidgets.ToggleButtons( description = 'augment:', options = ['Yes', 'No'] )\n",
    "\n",
    "display( augment_choice )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if augment_choice.value == 'Yes':    \n",
    "    with benchmark.CPU ( log, 'augmentation', compute_choice ) as run_context:        \n",
    "        if run_context.execute_block == True:\n",
    "            \n",
    "            data_cpu = teach_ML.augment_dataset_inplace ( data_cpu, data_dir, pandas )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if augment_choice.value == 'Yes':\n",
    "    with  benchmark.GPU ( log, 'augmentation', compute_choice ) as run_context:\n",
    "        if augment_choice.value == 'Yes' and run_context.execute_block == True:\n",
    "            \n",
    "            data = teach_ML.augment_dataset_inplace  ( data, data_dir, cudf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#8735fb'> 3. ETL </font>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset loaded, our goal is to prepare it for analysis and eventual ingestion by a Machine Learning model.\n",
    "\n",
    "To this end we need to come up with a strategy for handling missing data and for encoding non-numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> !BEWARE: \n",
    "* several of the code cells below delete and/or modify the dataset\n",
    "* running them more than once and/or out of order may break the expected downstream logic\n",
    "* its always possible to return to the ingestion stage to start with a fresh dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#8735fb'> 3.1. Handle Missing Data </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a very small fraction of flights have missing data (about 0.021% in 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns (not surprisingly in retrospect) that the dataset entries with missing values are almost exclusively instances of flight cancellations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those who are curious we invite you to explore further (e.g, ideas to get you started):\n",
    "```python\n",
    "data['Cancelled'].value_counts()\n",
    "data.dropna()['Cancelled'].value_counts()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cancelled flights are not relevant for our upstream prediction of arrival delays (i.e., cancelled flights can be labeled as being late without training a model), we feel comfortable dropping all data elements with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark.CPU ( log, 'ETL.dropna', compute_choice ) as run_context:        \n",
    "    if run_context.execute_block == True:\n",
    "        \n",
    "        data_cpu = data_cpu.dropna() # inplace drop samples w/ missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark.GPU ( log, 'ETL.dropna', compute_choice ) as run_context:        \n",
    "    if run_context.execute_block == True:\n",
    "        \n",
    "        data = data.dropna() # inplace drop samples w/ missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teach_ML.compare_speedups(log); # only for the most recent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#8735fb'> 3.2. Handle Non-Numeric Data </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some variables are numeric and easily lend themselves to being packaged into inputs for upstream modelling. \n",
    "\n",
    "Others (e.g., Origin, Dest, DestCityName, etc) are strings/non-numeric and will require a bit more effort to be included. \n",
    "\n",
    "We can enumerate the possible values of non-numeric features, and re-map them to integers using a category datatype conversion `.astype('category')`. To preseve the original data, we'll add/append the newly encoded version of the categorical features into columns with the `enc_` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings, mappings = data['OriginCityName'].factorize() # encode/categorize a sample feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list( zip ( data['OriginCityName'][0:10].values_host, encodings[0:10] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#8735fb'> 3.2.1 Encode and append </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark.CPU ( log, 'ETL.encode', compute_choice ) as run_context:\n",
    "    if run_context.execute_block == True:\n",
    "        for colname in data_cpu.columns:\n",
    "            if data_cpu[colname].dtype == object:\n",
    "\n",
    "                values = data_cpu[colname].astype('category').cat.codes.astype('float32') # encode\n",
    "                colname = 'enc_' + colname\n",
    "                \n",
    "                data_cpu.insert( len(data_cpu.columns), colname, values ) # add encoded column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark.GPU ( log, 'ETL.encode', compute_choice ) as run_context:\n",
    "    if run_context.execute_block == True:\n",
    "        for colname in data.columns:\n",
    "            if data[colname].dtype == object:\n",
    "                \n",
    "                values = data[colname].astype('category').cat.codes.astype('float32')\n",
    "                colname = 'enc_' + colname                \n",
    "                data.insert(0, colname, values)                \n",
    "                \n",
    "            numeric_columns += [ colname ]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#8735fb'> 3.3. Split Dataset into Train and Test </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='#8735fb'> 3.3.1. Filter Input Features </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = set ( ['ArrDel15'] ); \n",
    "target_surrogates = set ( ['ArrTime', 'ArrDelay'])\n",
    "\n",
    "redundant_cols = set( [ 'Year', 'Cancelled', 'DOT_ID_Reporting_Airline', 'enc_IATA_CODE_Reporting_Airline' ] )\n",
    "\n",
    "target_column = list ( target  )\n",
    "input_columns = list ( set( numeric_columns )\\\n",
    "                            .difference( target )\\\n",
    "                            .difference( target_surrogates )\\\n",
    "                            .difference( redundant_cols) )\n",
    "input_columns.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='#8735fb'> 3.3.2. Split (80%, 20%) <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark.CPU ( log, 'ETL.split', compute_choice ) as run_context:\n",
    "    if run_context.execute_block == True:\n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        X_train_cpu, X_test_cpu, \\\n",
    "            y_train_cpu, y_test_cpu = train_test_split( data_cpu[input_columns], \n",
    "                                                        data_cpu[target_column],\n",
    "                                                        train_size = 0.80, \n",
    "                                                        random_state = 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark.GPU ( log, 'ETL.split', compute_choice ) as run_context:\n",
    "    if run_context.execute_block == True:\n",
    "\n",
    "        train_test_split = cuml.preprocessing.model_selection.train_test_split\n",
    "\n",
    "        X_train, X_test, \\\n",
    "            y_train, y_test = train_test_split( X = data[input_columns],  \n",
    "                                                y = data[target_column], \n",
    "                                                train_size = 0.80, \n",
    "                                                random_state = 42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#8735fb'> 4. Explore </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the current state of our training data. We should have a dataframe with no missing values, and all numerical encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(5).to_pandas().round(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#8735fb'> 4.1. Cross-correlation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data science correlation matrix is frequently used to explain the strength of linear relationship between variables. Understanding the linear relationship between variables is useful because, if such relationship is existing, we can use the value of one variable to predict the value of the other variable.\n",
    "\n",
    "Computing cross-correlation in a large dataset is computationally expensive and yet easy to accelerate on GPU. \n",
    "<br>Here we can take advantage of [`CuPy`](https://cupy.chainer.org/) library which is GPU implementation of general-purpose array-processing library NumPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_columns = input_columns + target_column\n",
    "correlation_columns.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark.CPU ( log, 'explore.corrcoef', compute_choice ) as run_context:\n",
    "    if run_context.execute_block == True:\n",
    "\n",
    "         cov_cpu = numpy.corrcoef( data_cpu[ correlation_columns ].values, rowvar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark.GPU ( log, 'explore.corrcoef', compute_choice ) as run_context:\n",
    "    if run_context.execute_block == True:\n",
    "        \n",
    "        cov = cupy.corrcoef( data[ correlation_columns ].as_gpu_matrix(), rowvar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap( data = numpy.round( cupy.asnumpy(cov), 2),\n",
    "                  annot = True, linewidth = .5, \n",
    "                  cmap = sns.diverging_palette( 150, 275, s = 80 ),\n",
    "                  xticklabels = correlation_columns,\n",
    "                  yticklabels = correlation_columns,\n",
    "                  figure = plt.figure(figsize=(15,10)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take some time to observe the plot above.\n",
    "\n",
    "Values that you see in the cells are correlation coefficients that indicate how strong the relationship is. It ranges between -1 and 1. Closer it is to those extreme values, stronger the linear relationship between 2 variables is. \n",
    "You should notice that cells with higher numbers are having more intense color. This just makes it easier to find variables with stronger positive (purple) or negative (green) relationship. \n",
    "\n",
    "For your reference, you can see a clarification on the variables we used [here](https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236) \n",
    "\n",
    "You can note a few important observations from the plot above: \n",
    "1. Observe the variables that are highly corelated to our variable of interest: *ArrDel15*.<br/>\n",
    "You might have noticed that this binary variable has stronger linear relationship with other variables that indicate Arrival/Departure time and delay \n",
    "2. Perfect correlation between *ArrDelay* and *DepDelay* and strong relationship (0.7) between binary indicators *ArrDel15* and *DepDel15*.<br/>\n",
    "This tells us that if we know that one flight is about to depart earlier or later than scheduled, we can use this as a very strong indicator for arrival delay.\n",
    "High correlation coefficient between *ArrDel15* and *DepDel15* tells us that delayed flights are very likely to arrive to their destination late. \n",
    "3. There are many variables in this dataset that are not linearly related to each other. However, later in this tutorial we will show that advanced ML algorithms can still deduce useful and actionable information from this data even though simple ML Algorithms such as linear regression might fare poorly against lack of linear relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested to look into the code we used to generate the cross-correlation, you can uncomment and execute the cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#8735fb'> 4.2. Filter & Visualize </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['OriginCityName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = data [ data['OriginCityName'] == 'Seattle, WA' ]\n",
    "filtered['height'] = filtered['ArrDel15'] * 1 + filtered['ArrDelay'].scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = teach_ML.geo_plot( filtered )\n",
    "renderer.to_html( filename='geo.html', iframe_height = 500, iframe_width = 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = data [ data['OriginCityName'] == 'Atlanta, GA' ]\n",
    "filtered['height'] = filtered['ArrDel15'] * 1 + filtered['ArrDelay'].scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = teach_ML.geo_plot( filtered )\n",
    "renderer.to_html( filename='geo.html', iframe_height = 500, iframe_width = 1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#8735fb'> 5. Model </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flight delays are causing not just an inconvenience to passengers, but millions of dollars in damage to airlines and supporting businesses. \n",
    "Accurate prediction of flight delays can greatly reduce the economic loss caused by said delays. As an added benefit, travelers can get timely insights into potential issues with their future travel.<br/>\n",
    "\n",
    "We will use a year's worth of data to look for common patterns in late arrivals. These include factors like location, distance, airlines and whether the aircraft is departing late. \n",
    "We will use Random Forest to predict whether a flight will arrive on time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious, here is a helpful figure visualizing <a href='images/decision_tree_building.png'>how decisions trees are built</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#8735fb'> **Set Parameters**  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_rf = {\n",
    "    'n_estimators':10,\n",
    "    'max_depth':5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#8735fb'> **GPU Model Training and Inference** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_trained_model_cpu = None\n",
    "rf_trained_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with benchmark.GPU (log, 'rf.train', compute_choice ) as run_context:\n",
    "    if run_context.execute_block == True:\n",
    "        \n",
    "        from cuml.ensemble import RandomForestClassifier\n",
    "        \n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=model_params_rf['n_estimators'],\n",
    "            max_depth=model_params_rf['max_depth'],\n",
    "        )\n",
    "        trained_model = rf_model.fit(\n",
    "            X_train.astype('float32'), \n",
    "            y_train.astype('int32')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark.GPU (log, 'rf.inference', compute_choice ) as run_context:\n",
    "    if run_context.execute_block == True:\n",
    "        \n",
    "        predictions = trained_model.predict(\n",
    "            X_test.astype('float32')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#8735fb'> **CPU Model Training and Inference** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "with benchmark.CPU (log, 'rf.train', compute_choice ) as run_context:\n",
    "    if run_context.execute_block == True:\n",
    "        \n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        rf_model_cpu = RandomForestClassifier(\n",
    "            n_estimators=model_params_rf['n_estimators'],\n",
    "            max_depth=model_params_rf['max_depth'],\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        trained_model_cpu = rf_model_cpu.fit(\n",
    "            X_train_cpu.astype('float32'),\n",
    "            y_train_cpu.astype('int32')\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark.CPU (log, 'rf.inference', compute_choice ) as run_context:\n",
    "    if run_context.execute_block == True:\n",
    "        \n",
    "        predictions_cpu = trained_model_cpu.predict(\n",
    "            X_test_cpu.astype('float32')\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#8735fb'> Review Perf </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_ops = teach_ML.compare_speedups( log )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teach_ML.polar_plot_results( matched_ops, False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#8735fb'> References </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [Airline Dataset](https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236) \n",
    "\n",
    "> [RAPIDS website ](https://rapids.ai/) \n",
    "\n",
    "> [Forest Inference Library ](https://medium.com/rapids-ai/rapids-forest-inference-library-prediction-at-100-million-rows-per-second-19558890bc35)\n",
    "\n",
    "> [XGboost](https://xgboost.readthedocs.io/en/latest/)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "thanks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
