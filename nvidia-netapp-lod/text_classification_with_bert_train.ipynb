{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with BERT using Neural Modules - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State of the art Native Language Processing (NLP) uses large transformer models like BERT (Bidrectional Encoder Representations from Transformers) to extract meaningful representations from text. These models are pre-trained on a massive corpus of text using unsupervised methods to fill in randomly masked words. The pre-trained BERT model produces embeddings of the text input which then can be used in downstream tasks like text classification, question-answering, and named entity recognition.\n",
    "\n",
    "In this notebook your task will be text classification, specifically identifying sentences (and the phrases and words they contain) that express a positive or negative opinion. You will load a pre-trained BERT model and enhance it by performing additional training using data from the Stanford Sentiment Treebank. \n",
    "\n",
    "You will use NVIDIA Neural Modules (NeMo) to compose the text classification system. NeMo makes state of the art natural language understanding accessible and fast for data scientists. NeMo can automatically download pre-trained BERT models, use single-GPU or multi-GPU training, and leverages powerful optimization techniques like automatic mixed-precision (AMP). \n",
    "\n",
    "As part of this workflow, you will load, explore, and process the dataset. Then you will build scalable pipelines for gpu-based training. You will also see how to build inference pipelines that allow you to validate the quality of your models by visualizing the impact of fine-tuning on the learned BERT embeddings.\n",
    "\n",
    "This workflow also takes advantage the [NetApp AI Control Plane](https://blog.netapp.com/ai-control-plane) to perform data management tasks. In this notebook, you will see how to use the NetApp AI Control Plane to near-instantaneously save and restore versioned dataset and model baselines. You will see how to use this functionality to rollback to previous model versions and to implement dataset to model traceability in a simple manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLP Fine-tuning](figures/nlp_fine_tuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing all of the required modules/functions/classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo\n",
    "import nemo.collections.nlp as nemo_nlp\n",
    "from nemo.collections.nlp.data.datasets import BertTextClassificationDataset\n",
    "from nemo.collections.nlp.nm.data_layers.text_classification_datalayer import BertTextClassificationDataLayer\n",
    "from nemo.collections.nlp.nm.trainables import SequenceClassifier\n",
    "\n",
    "from nemo.backends.pytorch.common import CrossEntropyLossNM\n",
    "from nemo.utils.lr_policies import get_lr_policy\n",
    "from nemo.collections.nlp.callbacks.text_classification_callback import eval_iter_callback, eval_epochs_done_callback\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = -1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "from netapp_jupyter_utils import netappSnapshotCreate, netappGetSnapshots, netappRestoreSnapshot\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can safely ignore the \"UserWarning: Could not import torchaudio. Some features might not work.\" error this code generates, as this notebook does not utilize any \"torchaudio\" features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetApp Volume Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the names of the Kubernetes PersistentVolume (PV) objects on which the dataset and model reside. These models are presented as simple folders within the Jupyter workspace through the NetApp AI Control Plane, where the ./data directory is the mounted dataset PV and the ./model directory is the mounted model PV. These PVs were provisioned using NetApp Trident prior to the start of the lab, and are pre-populated with the lab's required dataset and model data.\n",
    "\n",
    "Notice that the NetApp storage constructs are abstracted away such that the user does not need to know or care that these directories actually represent mounted NetApp volumes, meaning data scientists aren't distracted by storage complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: PV = PersistentVolume\n",
    "datasetPvName = 'pvc-3d15d016-854b-4803-9ccc-033be4fa7fa9' # Kubernetes PV on which the dataset resides (mounted at ./data)\n",
    "modelPvName = 'pvc-b730748b-0510-4bda-9347-380364898f23' # Kubernetes PV on which trained models will be saved (mounted at ./model)\n",
    "\n",
    "# This just allows Jupyter to re-use these values in the next notebook\n",
    "%store datasetPvName\n",
    "%store modelPvName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [SST-2 dataset](https://nlp.stanford.edu/sentiment/index.html) is a standard benchmark for sentence classification and is part of the [GLUE Benchmark](https://gluebenchmark.com/tasks). As alluded to earlier, the SST-2 dataset has been pre-downloaded from GLUE and extracted into the \"data\" directory, which is actually a mounted NetApp volume.\n",
    "\n",
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = 'logs'\n",
    "DATA_DIR = 'data/SST-2'\n",
    "MODELS_DIR = 'models/'\n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_SEQ_LEN = 64 # we will pad with 0's shorter sentences and truncate longer\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR + '/train.tsv', sep='\\t')\n",
    "test_df = pd.read_csv(DATA_DIR + '/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first few lines of the dataset, which are sentences extracted from online movie reviews, to get a sense for what the dataset looks like. Each sentence has a label of 0 or 1, indicating whether that sentence is expressing a negative or positive sentiment, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes with a train file (containing labeled values) and a test file (containing unlabeled values).  You will use a portion of the train file contents for training and the rest for model validation. You also need to slightly reformat the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train to train and val and save to disk\n",
    "np.random.seed(123)\n",
    "train_mask = np.random.rand((len(df))) < .8\n",
    "train_df = df[train_mask]\n",
    "val_df = df[~train_mask]\n",
    "\n",
    "# Reformat test dataset (\"sentence\\tlabel\")\n",
    "test_df['label'] = 0\n",
    "test_df = test_df[['sentence', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the datasets to disk so that you can later take advantage of NetApp Snapshot technology to save baselined dataset versions. The data transformation you performed here was simple, but these transformations are often complex and may require experimentation with multiple augmentations. NetApp Snapshots facilitate that need by giving you the ability to save off different versions of datasets in a near-instantaneous and highly storage-efficient manner. You will create a snapshot in the next section of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new train, val, and test to disk\n",
    "SPLIT_DATA_DIR = os.path.join(DATA_DIR, 'split')\n",
    "\n",
    "os.makedirs(SPLIT_DATA_DIR, exist_ok=True)\n",
    "\n",
    "train_df.to_csv(os.path.join(SPLIT_DATA_DIR, 'train.tsv'), sep='\\t', index=False)\n",
    "val_df.to_csv(os.path.join(SPLIT_DATA_DIR, 'eval.tsv'), sep='\\t', index=False)\n",
    "test_df.to_csv(os.path.join(SPLIT_DATA_DIR, 'test.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataset Baseline for Traceability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, use NetApp Snapshot technology to near-instaneously save a baseline version of the dataset for traceability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a unique tag and a description for the training run. You will use these items to identify the snapshots that correspond to this specific training run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingRunTag = 'bert_%s' % datetime.today().strftime('%Y%m%d_%H%M%S') # Training run tag = bert_timestamp\n",
    "print('trainingRunTag: ', trainingRunTag)\n",
    "trainingRunDescription = 'BERT training run.'\n",
    "\n",
    "# This just allows Jupyter to re-use these values in the next notebook\n",
    "%store trainingRunTag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, trigger the creation of a Snapshot copy to save a baseline version of the dataset.\n",
    "\n",
    "A NetApp Snapshot copy is a read-only, point-in-time image of a volume. The image consumes minimal storage space and incurs negligible performance overhead because it only records file changes that have occurred since the last Snapshot was taken. This makes Snapshot copies ideal for saving versioned baselines of datasets and models, and for implementing dataset-to-model traceability. Since you are using the NetApp AI Control Plane, the underlying NetApp functionality is abstracted away, meaning you can trigger the creation of a Snapshot copy directly from within the notebook rather than having to navigate to an external NetApp-specific tool.\n",
    "\n",
    "If you want more details on how this NetApp integration works, the \"netappSnapshotCreate\" method is defined in the netapp_jupyter_utils.py helper script found in the same directory as this notebook. That script leverages ONTAP REST APIs to create the snapshot on the desired PV. If you want to examine that script more closely, you can open the JupyterLab File Browser and double-click that script file to open it in an editor, just be careful not to change the file's contents or you may break the script, which in turn would break the correct operation of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiResponse, snapshot = netappSnapshotCreate(pvName = datasetPvName, snapshotName = trainingRunTag, snapshotComment = trainingRunDescription)\n",
    "\n",
    "print('API Response: ', apiResponse['state'])\n",
    "print('Snapshot uuid: ', snapshot['uuid'])\n",
    "print('Snapshot name: ', snapshot['name'])\n",
    "print('Snapshot description: ', snapshot['comment'])\n",
    "print('Snapshot tag: ', trainingRunTag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NeMo, everything is a Neural Module. Neural modules abstract data and neural network architectures. A deep learning framework like PyTorch or Tensorflow combines neural network layers to create a neural network, while \n",
    "NeMo combines data and neural networks to create AI applications.\n",
    "The Neural Module Factory manages the neural modules, taking care to flow data through those modules, and it is also responsible for training (including mixed precision and distributed), logging, and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the neural module factory\n",
    "nf = nemo.core.NeuralModuleFactory(log_dir=WORK_DIR,\n",
    "                                   create_tb_writer=True,\n",
    "                                   add_time_to_log_dir=False,\n",
    "                                   optimization_level='O1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained models are automatically downloaded and cached. Watch your resource utilization tabs as this cell runs. You will see a spike in host network I/O bandwidth utilization, host CPU utilization will increase (but stay relatively low), and GPU memory consumption will increase as the model is loaded into GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained BERT\n",
    "bert = nemo_nlp.nm.trainables.huggingface.BERT(pretrained_model_name=PRETRAINED_MODEL_NAME)\n",
    "tokenizer = nemo_nlp.data.NemoBertTokenizer(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that the BERT models you are working with are massive. This gives the models a large capacity for learning, which is needed to understand the nuance and complexity of natural language. View the number of model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{PRETRAINED_MODEL_NAME} has {bert.num_weights} weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, traditional Multilayer Perceptron (MLP) neural network models are dramatically smaller. Define and instantiate the MLP feed forward network that takes the BERT embeddings as input and outputs the sentence classifications. This model has a much smaller footprint than the BERT model, as indicated by the smaller number of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp classifier\n",
    "bert_hidden_size = bert.hidden_size\n",
    "\n",
    "mlp = SequenceClassifier(hidden_size=bert_hidden_size, \n",
    "                         num_classes=2,\n",
    "                         num_layers=2,\n",
    "                         log_softmax=False,\n",
    "                         dropout=0.1)\n",
    "\n",
    "loss = CrossEntropyLossNM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared to the BERT model, the MLP is tiny.\n",
    "print(f'MLP has {mlp.num_weights} weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines define how data will flow through the different neural networks. In this case the data will flow through the BERT network and then the MLP network.\n",
    "\n",
    "Note that this specific use of the term \"Pipelines\" refers to the general concept of \"Pipelines\", not to the Kubeflow Pipelines framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a pre-built library to convert the data to a format that the BERT model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = BertTextClassificationDataLayer(input_file=os.path.join(SPLIT_DATA_DIR, 'train.tsv'),\n",
    "                                             tokenizer=tokenizer,\n",
    "                                             max_seq_length=MAX_SEQ_LEN,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             use_cache=True)\n",
    "\n",
    "val_data = BertTextClassificationDataLayer(input_file=os.path.join(SPLIT_DATA_DIR, 'eval.tsv'),\n",
    "                                           tokenizer=tokenizer,\n",
    "                                           max_seq_length=MAX_SEQ_LEN,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_token_types, train_attn_mask, train_labels = train_data()\n",
    "val_input, val_token_types, val_attn_mask, val_labels = val_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding and Visualizing BERT Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create BERT embeddings that convert words into a feature vector, and then use a machine learning algorithm called TSNE to separate a list of example words into categories representing \"good\" (i.e., positive) words and \"bad\" (i.e., negative) words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write example words to file\n",
    "spectrum_words = ['abysmal', 'apalling', 'dreadful', 'awful', 'terrible',\n",
    "                  'very bad', 'really bad', 'rubbish', 'unsatisfactory',\n",
    "                  'bad', 'poor', 'great', 'really good', 'very good', 'awesome'\n",
    "                  'fantastic', 'superb', 'brilliant', 'incredible', 'excellent'\n",
    "                  'outstanding', 'perfect']\n",
    "\n",
    "spectrum_file = os.path.join(SPLIT_DATA_DIR, 'positive_negative.tsv')\n",
    "with open(spectrum_file, 'w+') as f:\n",
    "    f.write('sentence\\tlabel')\n",
    "    for word in spectrum_words:\n",
    "        f.write('\\n' + word + '\\t0')\n",
    "\n",
    "spectrum_df = pd.read_csv(spectrum_file, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat data\n",
    "spectrum_data = BertTextClassificationDataLayer(input_file=spectrum_file,\n",
    "                                                tokenizer=tokenizer,\n",
    "                                                max_seq_length=MAX_SEQ_LEN,\n",
    "                                                batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word embeddings\n",
    "spectrum_input, spectrum_token_types, spectrum_attn_mask, spectrum_labels = spectrum_data()\n",
    "spectrum_embeddings = bert(input_ids=spectrum_input,\n",
    "                           token_type_ids=spectrum_token_types,\n",
    "                           attention_mask=spectrum_attn_mask)\n",
    "spectrum_embeddings_tensors = nf.infer(tensors=[spectrum_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a plot to visualize the words that the machine model classifies as expressing a \"good\" versus \"bad\" sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_activations = spectrum_embeddings_tensors[0][0][:,0,:].numpy()\n",
    "tsne_spectrum = TSNE(n_components=2, perplexity=10, verbose=1, learning_rate=2,\n",
    "                     random_state=123).fit_transform(spectrum_activations)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(tsne_spectrum[0:11, 0], tsne_spectrum[0:11, 1], 'rx')\n",
    "plt.plot(tsne_spectrum[11:, 0], tsne_spectrum[11:, 1], 'bo')\n",
    "for (x,y, label) in zip(tsne_spectrum[0:, 0], tsne_spectrum[0:, 1], spectrum_df.sentence.values.tolist() ):\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding plot, \"good\" words are denoted by a blue dot, and \"bad\" words are denoted by a red x. If the model is effective, then you should expect \"good\" words to be closely clustered together and \"bad\" words to be closely clustered together, with both cluster groups widely separated into the lower-left and upper-right corners. That is not what you see here, so clearly there is room to improve this model, which you will attempt to accomplish shortly through further training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Baseline Version of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, use NetApp Snapshot technology to save a baseline version of the model that you can revert back to in the event that you are not happy with the upcoming training results. Note that the size of a volume and the amount of data it contains does not factor into the time required to create an ONTAP snapshot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTag = 'baseline_' + trainingRunTag\n",
    "modelDescription = 'Baseline pre-trained BERT model.'\n",
    "\n",
    "bert.config.save_pretrained(MODELS_DIR)\n",
    "torch.save(bert.state_dict(), MODELS_DIR + 'pytorch_model.bin') # Save pre-trained model to volume\n",
    "\n",
    "apiResponse, snapshot = netappSnapshotCreate(pvName = modelPvName, snapshotName = modelTag, snapshotComment = modelDescription)\n",
    "\n",
    "print('API Response: ', apiResponse['state'])\n",
    "print('Snapshot uuid: ', snapshot['uuid'])\n",
    "print('Snapshot name: ', snapshot['name'])\n",
    "print('Snapshot description: ', snapshot['comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use BERT embeddings to convert from words to a feature vector. This time you are using the full dataset, not just a small sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = bert(input_ids=train_input,\n",
    "                        token_type_ids=train_token_types,\n",
    "                        attention_mask=train_attn_mask)\n",
    "val_embeddings = bert(input_ids=val_input,\n",
    "                      token_type_ids=val_token_types,\n",
    "                      attention_mask=val_attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizations and the loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "NUM_GPUS = 1\n",
    "LEARNING_RATE = 5e-5\n",
    "OPTIMIZER = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logits = mlp(hidden_states=train_embeddings)\n",
    "val_logits = mlp(hidden_states=val_embeddings)\n",
    "\n",
    "train_loss = loss(logits=train_logits, labels=train_labels)\n",
    "val_loss = loss(logits=val_logits, labels=val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks are used to record and log metrics and save checkpoints for the training and evaluation. This notebook also uses callbacks to print results to the display during training execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = len(train_data)\n",
    "steps_per_epoch = math.ceil(train_data_size / (BATCH_SIZE * NUM_GPUS))\n",
    "\n",
    "train_callback = nemo.core.SimpleLossLoggerCallback(tensors=[train_loss, train_logits],\n",
    "                            print_func=lambda x:nemo.logging.info(f'Train loss: {str(np.round(x[0].item(), 3))}'),\n",
    "                            tb_writer=nf.tb_writer,\n",
    "                            get_tb_values=lambda x: [[\"train_loss\", x[0]]],\n",
    "                            step_freq=steps_per_epoch)\n",
    "\n",
    "eval_callback = nemo.core.EvaluatorCallback(eval_tensors=[val_logits, val_labels],\n",
    "                                            user_iter_callback=lambda x, y: eval_iter_callback(x, y, val_data),\n",
    "                                            user_epochs_done_callback=lambda x:\n",
    "                                                eval_epochs_done_callback(x, f'{nf.work_dir}/graphs'),\n",
    "                                            tb_writer=nf.tb_writer,\n",
    "                                            eval_step=steps_per_epoch)\n",
    "\n",
    "# Create callback to save checkpoints\n",
    "ckpt_callback = nemo.core.CheckpointCallback(folder=nf.checkpoint_dir,\n",
    "                                             epoch_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_policy_fn = get_lr_policy('WarmupAnnealing',\n",
    "                             total_steps=NUM_EPOCHS * steps_per_epoch,\n",
    "                             warmup_ratio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the neural network. This may take up to 5 minutes. If you set up your notebook dashboard with the resource utilization graphs as described in the accompanying lab guide, then you should see GPU Memory utilization graph max out and hold, and GPU Utilization should ramp up, hold, and then drop multiple times as the training process executes each of the model's three epochs. Host CPU utilization will stat relatively low throughout as the GPU is performing most of the training work.\n",
    "\n",
    "At the beginning of training and at the end of each of the three training epochs, a callback prints a confusion matrix summarizing that epoch's accuracy results, four matrices in total, with values normalized to the range 0.00 to 1.00. The initial matrix will report accuracy around 0.56, while matrices for later epochs will produce around 0.95, reflecting improved model accuracy.\n",
    "\n",
    "Once training completes, the cell will output four graphical four-square color confusion matrices summarizing these same results. Purple squares in the lower-left and upper-right quadrants represent a more accurate model. While the first graphic does not match this scheme, later ones do, again indicating that model accuracy has improved during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run Training\n",
    "nf.train(tensors_to_optimize=[train_loss],\n",
    "         callbacks=[train_callback, eval_callback, ckpt_callback],\n",
    "         lr_policy=lr_policy_fn,\n",
    "         optimizer=OPTIMIZER,\n",
    "         optimization_params={'num_epochs': NUM_EPOCHS, 'lr': LEARNING_RATE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pause here until the training finishes, as indicated by the contents of the square brackets located to the left of the preceding code cell changing from a '\\*' character to a number. The code cell also outputs a series of colored four-square charts when it finishes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Conclusion\n",
    "Congratulations! You have just created a Deep Learning model using the BERT architecture and training. You loaded pre-trained weights from an  online model registry, such as NVIDIA's [NGC](http://ngc.nvidia.com/). You fine-tuned the model for a specific use-case using a real-world dataset, and you performed a basic analysis on the quality of the model.\n",
    "\n",
    "If you examine your resource utilization tabs, they show that the GPU is idle because the training process has completed. Meanwhile, the GPU Memory is still fully utilized because the model and the weights for the model are still loaded in memory, so that if you needed to perform more training you would not have to re-load this data into GPU RAM again.\n",
    "\n",
    "Shut down the Jupyter Kernel, which will clear out the system resources, including the GPU memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook you will reload this enhanced model and use it to perform an inference.\n",
    "\n",
    "Please refer back to the lab manual and continue on to the next notebook when you are ready.\n",
    "\n",
    "### [Click Here for Notebook 2 - Inference](text_classification_with_bert_inference.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
