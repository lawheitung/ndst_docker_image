{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with BERT using Neural Modules - Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You must run the \"Text Classification with BERT using Neural Modules - Training\" notebook before running this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training notebook you leveraged Neural Modules (NeMo) to train a state of the art NLP model (BERT). You started with a model that had been pre-trained on a massive corpus of text and assessed the performance against a trivial dataset. You then used the SST-2 dataset to fine-tune the model to perform a specific use case. In that notebook this training took a few minutes over a small number of epochs, but with real-world applications and larger datasets this fine-tuning can take hours or days.\n",
    "\n",
    "During the training process you also leveraged NetApp storage to take Snapshots of the trained model checkpoints.\n",
    "\n",
    "In the context of Artificial Intelligence and Machine Learning, inference refers to applying a trained model to real-world data that was not part of the training in order to make a prediction based on that data. For inference, you instantiate the same neural models you used for training, but using the checkpoints that captured the enhanced model that captures the training results.\n",
    "\n",
    "In this notebook you will re-instantiate the enhanced model using the same model architecture and the model weights and checkpoints that were learned during training. You will then utilize the [NetApp AI Control Plane](https://blog.netapp.com/ai-control-plane) to perform data management tasks to assess and compare the results of the untrained and enhanced model versions. Lastly you will investigate how to use this functionality to implement traceability and rollbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLP Fine-tuning](figures/nlp_fine_tuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all required modules/functions/classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo\n",
    "import nemo.collections.nlp as nemo_nlp\n",
    "from nemo.collections.nlp.data.datasets import BertTextClassificationDataset\n",
    "from nemo.collections.nlp.nm.data_layers.text_classification_datalayer import BertTextClassificationDataLayer\n",
    "from nemo.collections.nlp.nm.trainables import SequenceClassifier\n",
    "\n",
    "from nemo.backends.pytorch.common import CrossEntropyLossNM\n",
    "from nemo.utils.lr_policies import get_lr_policy\n",
    "from nemo.collections.nlp.callbacks.text_classification_callback import eval_iter_callback, eval_epochs_done_callback\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = -1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "from netapp_jupyter_utils import netappSnapshotCreate, netappGetSnapshots, netappRestoreSnapshot\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, you can safely ignore any warnings regarding \"torchaudio\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps set up and execute the inference pipeline using the real-world data set you will be working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the model. If you watch your utilization tabs you will see GPU memory utilization increase somewhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to data, checkpoints, logs, and model files\n",
    "WORK_DIR = 'logs'\n",
    "DATA_DIR = 'data/SST-2'\n",
    "SPLIT_DATA_DIR = os.path.join(DATA_DIR, 'split')\n",
    "MODELS_DIR = 'models/'\n",
    "\n",
    "# instantiate the neural module factory\n",
    "nf = nemo.core.NeuralModuleFactory(log_dir=WORK_DIR,\n",
    "                                   create_tb_writer=True,\n",
    "                                   add_time_to_log_dir=False,\n",
    "                                   optimization_level='O1')\n",
    "\n",
    "# Read in saved checkpoint/volume information\n",
    "%store -r datasetPvName\n",
    "%store -r modelPvName\n",
    "%store -r trainingRunTag\n",
    "\n",
    "# Pre-trained BERT model, simple classifier, and tokenizer\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_SEQ_LEN = 64 # we will pad with 0's shorter sentences and truncate longer\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "bert = nemo_nlp.nm.trainables.huggingface.BERT(pretrained_model_name=PRETRAINED_MODEL_NAME)\n",
    "bert_hidden_size = bert.hidden_size\n",
    "tokenizer = nemo_nlp.data.NemoBertTokenizer(PRETRAINED_MODEL_NAME)\n",
    "mlp = SequenceClassifier(hidden_size=bert_hidden_size, \n",
    "                         num_classes=2,\n",
    "                         num_layers=2,\n",
    "                         log_softmax=False,\n",
    "                         dropout=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you load the trained checkpoints, you will see GPU memory utilization increase further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.restore_from('logs/checkpoints/BERT-EPOCH-3.pt')\n",
    "mlp.restore_from('logs/checkpoints/SequenceClassifier-EPOCH-3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Sentence Classification Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the function you will use to perform a quick inference on a small set of new sentences. The inference will predict whether these sentences are expressing positive or negative opinions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(nf, tokenizer, bert, mlp, sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tmp_file = \"/tmp/tmp_sentence.tsv\"\n",
    "    with open(tmp_file, 'w+') as tmp_tsv:\n",
    "        header = 'sentence\\tlabel\\n'\n",
    "        line = sentence + '\\t0\\n'\n",
    "        tmp_tsv.writelines([header, line])\n",
    "\n",
    "    tmp_data = BertTextClassificationDataLayer(input_file=tmp_file,\n",
    "                                               tokenizer=tokenizer,\n",
    "                                               max_seq_length=128,\n",
    "                                               batch_size=1)\n",
    "    \n",
    "    tmp_input, tmp_token_types, tmp_attn_mask, _ = tmp_data()\n",
    "    tmp_embeddings = bert(input_ids=tmp_input,\n",
    "                          token_type_ids=tmp_token_types,\n",
    "                          attention_mask=tmp_attn_mask)\n",
    "    tmp_logits = mlp(hidden_states=tmp_embeddings)\n",
    "    tmp_logits_tensors = nf.infer(tensors=[tmp_logits, tmp_embeddings])\n",
    "    tmp_probs = torch.nn.functional.softmax(torch.cat(tmp_logits_tensors[0])).numpy()[:, 1] \n",
    "    print(f'{sentence} | {tmp_probs[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the inference. If you get a deprecation warning during this execution, you can safely ignore it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['point break is the best movie of all time',\n",
    "             'the movie was a wonderful exercise in understanding the struggles of native americans',\n",
    "             'the performance of diego luna had me excited and annoyed at the same time',\n",
    "             'matt damon is the only good thing about this film']\n",
    "\n",
    "for sentence in sentences:\n",
    "    classify_sentence(nf, tokenizer, bert, mlp, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the results of each sentence classification, scan the output results looking for the lines that match the following pattern.\n",
    "\n",
    "\\[NeMo I \\<date> \\<time> actions:728\\] Evaluating batch 0 out of 1\n",
    "\n",
    "There should be a line like this for each of the four sentences listed in the \"sentences\" array defined at the beginning of the code cell that generated this output.\n",
    "\n",
    "The line immediately following these lines will contain the sentence text, a pipe character, and then a decimal number between 0 and 1 indicating the model's interpretation of the sentiment expressed by that sentence. A value close to 0 indicates a strongly negative sentiment, while a value close to 1 indicates a strongly positive sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing BERT Embeddings After Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a fine-tuned BERT model, you will run the same assessment you ran during training, this time using the enhanced model. You will see if it produces an improved plot of the \"good\" and \"bad\" sample words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_words = ['abysmal', 'apalling', 'dreadful', 'awful', 'terrible',\n",
    "                  'very bad', 'really bad', 'rubbish', 'unsatisfactory',\n",
    "                  'bad', 'poor', 'great', 'really good', 'very good', 'awesome'\n",
    "                  'fantastic', 'superb', 'brilliant', 'incredible', 'excellent'\n",
    "                  'outstanding', 'perfect']\n",
    "\n",
    "spectrum_file = os.path.join(SPLIT_DATA_DIR, 'positive_negative.tsv')\n",
    "with open(spectrum_file, 'w+') as f:\n",
    "    f.write('sentence\\tlabel')\n",
    "    for word in spectrum_words:\n",
    "        f.write('\\n' + word + '\\t0')\n",
    "        \n",
    "spectrum_df = pd.read_csv(spectrum_file, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat text\n",
    "spectrum_data = BertTextClassificationDataLayer(input_file=spectrum_file,\n",
    "                                                tokenizer=tokenizer,\n",
    "                                                max_seq_length=MAX_SEQ_LEN,\n",
    "                                                batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trained model to create embeddings\n",
    "\n",
    "spectrum_input, spectrum_token_types, spectrum_attn_mask, spectrum_labels = spectrum_data()\n",
    "\n",
    "spectrum_embeddings = bert(input_ids=spectrum_input,\n",
    "                           token_type_ids=spectrum_token_types,\n",
    "                           attention_mask=spectrum_attn_mask)\n",
    "\n",
    "spectrum_embeddings_tensors = nf.infer(tensors=[spectrum_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a better-trained model, you should expect to see more clustering of \"good\" and \"bad\" words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_activations = spectrum_embeddings_tensors[0][0][:,0,:].numpy()\n",
    "tsne_spectrum = TSNE(n_components=2, perplexity=10, verbose=1, learning_rate=2,\n",
    "                     random_state=123).fit_transform(spectrum_activations)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(tsne_spectrum[0:11, 0], tsne_spectrum[0:11, 1], 'rx')\n",
    "plt.plot(tsne_spectrum[11:, 0], tsne_spectrum[11:, 1], 'bo')\n",
    "for (x,y, label) in zip(tsne_spectrum[0:, 0], tsne_spectrum[0:, 1], spectrum_df.sentence.values.tolist() ):\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new plot show significant clustering of the \"good\" words in the far lower-left corner, and signficant clustering of the \"bad\" words in the far upper-right corner. The enhanced model is doing a much better job of identifying and distinguishng \"good\" and \"bad\" words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save New Model Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use NetApp Snapshot technology to near-instaneously save a new version of the enhanced model so that you will be able to revert back to it in the future if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTag = 'enhanced_' + trainingRunTag\n",
    "modelDescription = 'Enhanced BERT model.'\n",
    "\n",
    "bert.config.save_pretrained(MODELS_DIR)\n",
    "torch.save(bert.state_dict(), MODELS_DIR + 'pytorch_model.bin') # Save pre-trained model to volume\n",
    "\n",
    "apiResponse, snapshot = netappSnapshotCreate(pvName = modelPvName, snapshotName = modelTag, snapshotComment = modelDescription)\n",
    "\n",
    "print('API Response: ', apiResponse['state'])\n",
    "print('Snapshot uuid: ', snapshot['uuid'])\n",
    "print('Snapshot name: ', snapshot['name'])\n",
    "print('Snapshot description: ', snapshot['comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Saved Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all of the model versions that you have saved using NetApp Snapshot technology. If for some reason you weren't happy with the results of the newly-trained model, you can always use the snapshot to quickly revert back to one of the previous model versions.\n",
    "\n",
    "Note that you could also use NetApp FlexClone technology to clone any one of these saved models in order to experiment with it in a sandboxed workspace. While this capability is not demonstrated in this lab, you can refer to the [NetApp AI Control Plane Technical Report](https://www.netapp.com/us/media/tr-4798.pdf) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = netappGetSnapshots(pvName = modelPvName)\n",
    "\n",
    "# Print list of snapshots\n",
    "print('Model Tag', '\\t\\t\\t', 'Model Description')\n",
    "for snapshot in snapshots :\n",
    "    try:\n",
    "        print(snapshot['name'], '\\t', snapshot['comment'])\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly enhanced model is clearly more accurate than the baseline model, as demonstrated by the clustering of \"good\" and \"bad\" words in the above plot. However, \"enhanced\" models are not always more accurate than previously-trained models, so assume that this model is actually less accurate than a previous baseline version, and that you want to restore that previous model. To demonstrate this procedure, you will now use NetApp technology to quickly restore the previous baseline model.\n",
    "\n",
    "Note that restoring a volume to a snapshot destroys any existing snapshots that were taken later on the volume. In this example you are going to restore the volume to the snapshot corresponding to the model's baseline, which will destroy the snapshot of the enhanced model that you just took in the preceding step. If you needed to retain those later snapshots, NetApp FlexClone technology can again help you. While this version of the lab does not include a demonstration of FlexClone, we intend to include it in a future version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the Previously Saved Model\n",
    "modelTag = 'baseline_' + trainingRunTag # Model Tag of the model that we wish to restore\n",
    "print('Restoring: ', modelTag)\n",
    "\n",
    "# Restore NetApp snapshot corresponding to model version\n",
    "result = netappRestoreSnapshot(pvName = modelPvName, snapshotName = modelTag)\n",
    "print(result)\n",
    "\n",
    "# Reload model\n",
    "bert = nemo_nlp.nm.trainables.huggingface.BERT(pretrained_model_name = MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-visualize BERT Embeddings after Restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the BERT embeddings once again to confirm that you have reloaded the baseline (non-enhanced) BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_words = ['abysmal', 'apalling', 'dreadful', 'awful', 'terrible',\n",
    "                  'very bad', 'really bad', 'rubbish', 'unsatisfactory',\n",
    "                  'bad', 'poor', 'great', 'really good', 'very good', 'awesome'\n",
    "                  'fantastic', 'superb', 'brilliant', 'incredible', 'excellent'\n",
    "                  'outstanding', 'perfect']\n",
    "\n",
    "spectrum_file = os.path.join(SPLIT_DATA_DIR, 'positive_negative.tsv')\n",
    "with open(spectrum_file, 'w+') as f:\n",
    "    f.write('sentence\\tlabel')\n",
    "    for word in spectrum_words:\n",
    "        f.write('\\n' + word + '\\t0')\n",
    "        \n",
    "spectrum_df = pd.read_csv(spectrum_file, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat text\n",
    "spectrum_data = BertTextClassificationDataLayer(input_file=spectrum_file,\n",
    "                                                tokenizer=tokenizer,\n",
    "                                                max_seq_length=MAX_SEQ_LEN,\n",
    "                                                batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trained model to create embeddings\n",
    "\n",
    "spectrum_input, spectrum_token_types, spectrum_attn_mask, spectrum_labels = spectrum_data()\n",
    "\n",
    "spectrum_embeddings = bert(input_ids=spectrum_input,\n",
    "                           token_type_ids=spectrum_token_types,\n",
    "                           attention_mask=spectrum_attn_mask)\n",
    "\n",
    "spectrum_embeddings_tensors = nf.infer(tensors=[spectrum_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have reloaded the original model, you should expect the plot to once again show less clustering than you saw in the plot for the enhanced model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_activations = spectrum_embeddings_tensors[0][0][:,0,:].numpy()\n",
    "tsne_spectrum = TSNE(n_components=2, perplexity=10, verbose=1, learning_rate=2,\n",
    "                     random_state=123).fit_transform(spectrum_activations)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(tsne_spectrum[0:11, 0], tsne_spectrum[0:11, 1], 'rx')\n",
    "plt.plot(tsne_spectrum[11:, 0], tsne_spectrum[11:, 1], 'bo')\n",
    "for (x,y, label) in zip(tsne_spectrum[0:, 0], tsne_spectrum[0:, 1], spectrum_df.sentence.values.tolist() ):\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words are no longer clustered in this plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the notebook activities. Please refer to the lab guide again to complete the remainder of the lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
